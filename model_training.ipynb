{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import spacy\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login, Repository\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "LR = 5e-5\n",
    "EPS = 1e-8\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "ACCUMULATION_STEPS = 4\n",
    "MAX_LEN = 128\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Log in to Hugging Face\n",
    "login(token='YOUR_HF_TOKEN')  # Replace 'YOUR_HF_TOKEN' with your actual Hugging Face token\n",
    "\n",
    "# Load dataset from Hugging Face\n",
    "dataset = load_dataset('onekat/literalmente-dataset')\n",
    "\n",
    "# Concatenate splits into a single DataFrame for processing\n",
    "df_train = pd.DataFrame(dataset['train'])\n",
    "df_validation = pd.DataFrame(dataset['validation'])\n",
    "df_test = pd.DataFrame(dataset['test'])\n",
    "\n",
    "df = pd.concat([df_train, df_validation, df_test], ignore_index=True)\n",
    "\n",
    "# Assuming labels are as follows:\n",
    "# 0: Etymological\n",
    "# 1: Intensifier\n",
    "# 2: Emphatic\n",
    "\n",
    "# Map label indices to label names\n",
    "label_names = {0: 'Etymological', 1: 'Intensifier', 2: 'Emphatic'}\n",
    "df['sentido'] = df['label'].map(label_names)\n",
    "\n",
    "# Load spaCy and extract sentences containing 'literalmente'\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "def extract_literalmente_sentence(text):\n",
    "    doc = nlp(text)\n",
    "    sentences_with_literalmente = []\n",
    "    for sent in doc.sents:\n",
    "        if 'literalmente' in sent.text.lower():\n",
    "            sentences_with_literalmente.append(sent.text.strip())\n",
    "    return ' '.join(sentences_with_literalmente)\n",
    "\n",
    "# Apply the function\n",
    "df['text_literalmente'] = df['text'].apply(extract_literalmente_sentence)\n",
    "\n",
    "# Remove rows without sentences containing 'literalmente'\n",
    "df = df[df['text_literalmente'].str.strip() != '']\n",
    "\n",
    "model_label_configs = {\n",
    "    'Model_1': {\n",
    "        'labels_to_include': ['Etymological', 'Intensifier', 'Emphatic'],\n",
    "        'label_mapping': {'Etymological': 0, 'Intensifier': 1, 'Emphatic': 2}\n",
    "    },\n",
    "    'Model_2': {\n",
    "        'labels_to_include': ['Intensifier', 'Emphatic'],\n",
    "        'label_mapping': {'Intensifier': 0, 'Emphatic': 1}\n",
    "    },\n",
    "    'Model_3': {\n",
    "        'labels_to_include': ['Etymological', 'Intensifier'],\n",
    "        'label_mapping': {'Etymological': 0, 'Intensifier': 1}\n",
    "    },\n",
    "    'Model_4': {\n",
    "        'labels_to_include': ['Etymological', 'Emphatic'],\n",
    "        'label_mapping': {'Etymological': 0, 'Emphatic': 1}\n",
    "    },\n",
    "    'Model_5': {\n",
    "        'labels_to_include': ['Etymological', 'Intensifier', 'Emphatic'],\n",
    "        'label_mapping': {'Etymological': 0, 'Intensifier': 1, 'Emphatic': 1}\n",
    "    },\n",
    "    'Model_6': {\n",
    "        'labels_to_include': ['Etymological', 'Intensifier', 'Emphatic'],\n",
    "        'label_mapping': {'Etymological': 0, 'Intensifier': 1, 'Emphatic': 0}\n",
    "    },\n",
    "}\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased', do_lower_case=True)\n",
    "\n",
    "def tokenize_data(sentences, labels):\n",
    "    inputs = tokenizer(\n",
    "        list(sentences),\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LEN,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return inputs['input_ids'], inputs['attention_mask'], torch.tensor(labels)\n",
    "\n",
    "def create_dataloader(inputs, masks, labels, sampler):\n",
    "    data = TensorDataset(inputs, masks, labels)\n",
    "    return DataLoader(data, sampler=sampler(data), batch_size=BATCH_SIZE)\n",
    "\n",
    "def train_and_evaluate(model_name, label_config):\n",
    "    print(f\"\\nTraining {model_name}...\\n\")\n",
    "    df_model = df.copy()\n",
    "    df_model = df_model[df_model['sentido'].isin(label_config['labels_to_include'])]\n",
    "    df_model['label_mapped'] = df_model['sentido'].map(label_config['label_mapping'])\n",
    "    df_model.dropna(subset=['text_literalmente', 'label_mapped'], inplace=True)\n",
    "    sentences = df_model['text_literalmente'].tolist()\n",
    "    labels = df_model['label_mapped'].tolist()\n",
    "    temp_df = pd.DataFrame({'text': sentences, 'label': labels})\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_resampled, y_resampled = ros.fit_resample(temp_df[['text']], temp_df['label'])\n",
    "    sentences_resampled = X_resampled['text'].tolist()\n",
    "    labels_resampled = y_resampled.tolist()\n",
    "    sentences_resampled = np.array(sentences_resampled)\n",
    "    labels_resampled = np.array(labels_resampled)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        sentences_resampled, labels_resampled, test_size=0.2, random_state=42, stratify=labels_resampled)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "    print(f\"Number of training samples: {len(X_train)}\")\n",
    "    print(f\"Number of validation samples: {len(X_val)}\")\n",
    "    print(f\"Number of test samples: {len(X_test)}\")\n",
    "    train_inputs, train_masks, train_labels = tokenize_data(X_train, y_train)\n",
    "    val_inputs, val_masks, val_labels = tokenize_data(X_val, y_val)\n",
    "    test_inputs, test_masks, test_labels = tokenize_data(X_test, y_test)\n",
    "    train_dataloader = create_dataloader(train_inputs, train_masks, train_labels, RandomSampler)\n",
    "    val_dataloader = create_dataloader(val_inputs, val_masks, val_labels, SequentialSampler)\n",
    "    test_dataloader = create_dataloader(test_inputs, test_masks, test_labels, SequentialSampler)\n",
    "    num_labels = len(set(label_config['label_mapping'].values()))\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        'dccuchile/bert-base-spanish-wwm-uncased',\n",
    "        num_labels=num_labels,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "    )\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=LR, eps=EPS)\n",
    "    total_steps = len(train_dataloader) * EPOCHS // ACCUMULATION_STEPS\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    best_val_accuracy = 0\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False)\n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            batch_input_ids = batch[0].to(device)\n",
    "            batch_input_mask = batch[1].to(device)\n",
    "            batch_labels = batch[2].to(device)\n",
    "            outputs = model(batch_input_ids, attention_mask=batch_input_mask, labels=batch_labels)\n",
    "            loss = outputs.loss / ACCUMULATION_STEPS\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            if (step + 1) % ACCUMULATION_STEPS == 0 or (step + 1) == len(train_dataloader):\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            progress_bar.set_postfix({'Loss': loss.item() * ACCUMULATION_STEPS})\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        model.eval()\n",
    "        val_preds, val_labels_list = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                batch_input_ids = batch[0].to(device)\n",
    "                batch_input_mask = batch[1].to(device)\n",
    "                batch_labels = batch[2].to(device)\n",
    "                outputs = model(batch_input_ids, attention_mask=batch_input_mask)\n",
    "                logits = outputs.logits\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels_list.extend(batch_labels.cpu().numpy())\n",
    "        val_accuracy = np.mean(np.array(val_preds) == np.array(val_labels_list))\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS} - Average Training Loss: {avg_train_loss:.4f} - Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            epochs_no_improve = 0\n",
    "            model_name_hub = f'onekat/{model_name}'\n",
    "            # Prepare the model card content\n",
    "            inverse_label_mapping = {v: k for k, v in label_config['label_mapping'].items()}\n",
    "            target_names = [inverse_label_mapping[i] for i in range(num_labels)]\n",
    "            # Generate classification report on validation set\n",
    "            val_classification_report = classification_report(\n",
    "                val_labels_list, val_preds, target_names=target_names, output_dict=True\n",
    "            )\n",
    "            # Prepare the model card metadata\n",
    "            model_card = f\"\"\"\n",
    "            # {model_name}\n",
    "\n",
    "            This model is a fine-tuned version of [dccuchile/bert-base-spanish-wwm-uncased](https://huggingface.co/dccuchile/bert-base-spanish-wwm-uncased) on a custom dataset.\n",
    "\n",
    "            ## Hyperparameters\n",
    "\n",
    "            - Learning Rate: {LR}\n",
    "            - Epsilon: {EPS}\n",
    "            - Epochs: {epoch + 1}\n",
    "            - Batch Size: {BATCH_SIZE}\n",
    "            - Gradient Accumulation Steps: {ACCUMULATION_STEPS}\n",
    "            - Max Sequence Length: {MAX_LEN}\n",
    "\n",
    "            ## Evaluation Results\n",
    "\n",
    "            **Validation Accuracy:** {val_accuracy:.4f}\n",
    "\n",
    "            ### Classification Report on Validation Set\n",
    "\n",
    "            \"\"\"\n",
    "            for label in target_names:\n",
    "                precision = val_classification_report[label]['precision']\n",
    "                recall = val_classification_report[label]['recall']\n",
    "                f1_score = val_classification_report[label]['f1-score']\n",
    "                support = val_classification_report[label]['support']\n",
    "                model_card += f\"- **{label}**\\n\"\n",
    "                model_card += f\"  - Precision: {precision:.4f}\\n\"\n",
    "                model_card += f\"  - Recall: {recall:.4f}\\n\"\n",
    "                model_card += f\"  - F1-score: {f1_score:.4f}\\n\"\n",
    "                model_card += f\"  - Support: {support}\\n\\n\"\n",
    "\n",
    "            # Save the model card to a file\n",
    "            with open(\"README.md\", \"w\") as f:\n",
    "                f.write(model_card)\n",
    "                    # Save the model card to the model directory\n",
    "\n",
    "           # Initialize or clone the repository\n",
    "            repo = Repository(local_dir=model_name, clone_from=model_name_hub, use_auth_token='YOUR_HF_TOKEN')\n",
    "\n",
    "            # Add all files and commit\n",
    "            repo.git_add()\n",
    "            repo.git_commit(f\"Update model after epoch {epoch + 1}\")\n",
    "\n",
    "            # Push to the Hub\n",
    "            repo.git_push()\n",
    "\n",
    "            print(f\"Model and README.md pushed to Hugging Face Hub at epoch {epoch + 1} with validation accuracy {val_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "            # Push the model to Hugging Face Hub with the model card\n",
    "            model.push_to_hub(model_name_hub, token='YOUR_HF_TOKEN')\n",
    "            tokenizer.push_to_hub(model_name_hub, token='YOUR_HF_TOKEN')\n",
    "            print(f\"Model pushed to Hugging Face Hub at epoch {epoch + 1} with validation accuracy {val_accuracy:.4f}\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"No improvement in validation accuracy in the last {EARLY_STOPPING_PATIENCE} epochs. Stopping training.\")\n",
    "                break\n",
    "    print(f\"Training of {model_name} completed.\")\n",
    "    # Final evaluation on the test set\n",
    "    model.eval()\n",
    "    test_preds, test_labels_list = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            batch_input_ids = batch[0].to(device)\n",
    "            batch_input_mask = batch[1].to(device)\n",
    "            batch_labels = batch[2].to(device)\n",
    "            outputs = model(batch_input_ids, attention_mask=batch_input_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            test_preds.extend(preds.cpu().numpy())\n",
    "            test_labels_list.extend(batch_labels.cpu().numpy())\n",
    "    # Generate classification report on test set\n",
    "    test_classification_report = classification_report(\n",
    "        test_labels_list, test_preds, target_names=target_names, output_dict=True\n",
    "    )\n",
    "    test_accuracy = np.mean(np.array(test_preds) == np.array(test_labels_list))\n",
    "    # Update the model card with test results\n",
    "    model_card += f\"## Test Results\\n\\n**Test Accuracy:** {test_accuracy:.4f}\\n\\n### Classification Report on Test Set\\n\\n\"\n",
    "    for label in target_names:\n",
    "        precision = test_classification_report[label]['precision']\n",
    "        recall = test_classification_report[label]['recall']\n",
    "        f1_score = test_classification_report[label]['f1-score']\n",
    "        support = test_classification_report[label]['support']\n",
    "        model_card += f\"- **{label}**\\n\"\n",
    "        model_card += f\"  - Precision: {precision:.4f}\\n\"\n",
    "        model_card += f\"  - Recall: {recall:.4f}\\n\"\n",
    "        model_card += f\"  - F1-score: {f1_score:.4f}\\n\"\n",
    "        model_card += f\"  - Support: {support}\\n\\n\"\n",
    "    # Save the updated model card\n",
    "    with open(\"README.md\", \"w\") as f:\n",
    "        f.write(model_card)\n",
    "    # Push the updated model card to the hub\n",
    "    model.push_to_hub(model_name_hub, token='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')\n",
    "    tokenizer.push_to_hub(model_name_hub, token='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')\n",
    "    print(f\"Updated model card with test results pushed to Hugging Face Hub for {model_name}.\")\n",
    "\n",
    "for model_name, label_config in model_label_configs.items():\n",
    "    train_and_evaluate(model_name, label_config)\n",
    "\n",
    "print(\"Training of all models completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
