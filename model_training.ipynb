{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "LR = 5e-5\n",
    "EPS = 1e-8\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "ACCUMULATION_STEPS = 4\n",
    "MAX_LEN = 128\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Log in to Hugging Face\n",
    "login(token='YOUR_HF_TOKEN')  # Replace with your actual Hugging Face token\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset('onekat/lit-dataset')\n",
    "\n",
    "# Extract the only split\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "# Ensure correct label mappings\n",
    "label_names = {0: 'Intensifier', 1: 'Emphatic', 2: 'Etymological'}\n",
    "df['sentido'] = df['label'].map(label_names)\n",
    "\n",
    "# Load spaCy and extract sentences containing 'literalmente'\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "def extract_literalmente_sentence(text):\n",
    "    doc = nlp(text)\n",
    "    sentences_with_literalmente = []\n",
    "    for sent in doc.sents:\n",
    "        if 'literalmente' in sent.text.lower():\n",
    "            sentences_with_literalmente.append(sent.text.strip())\n",
    "    return ' '.join(sentences_with_literalmente)\n",
    "\n",
    "df['text_literalmente'] = df['text'].apply(extract_literalmente_sentence)\n",
    "\n",
    "# Remove rows without 'literalmente'\n",
    "df = df[df['text_literalmente'].str.strip() != '']\n",
    "\n",
    "# Model configurations\n",
    "model_label_configs = {\n",
    "    'Model_1': {'labels_to_include': ['Etymological', 'Intensifier', 'Emphatic'], 'label_mapping': {'Etymological': 2, 'Intensifier': 0, 'Emphatic': 1}},\n",
    "    'Model_2': {'labels_to_include': ['Intensifier', 'Emphatic'], 'label_mapping': {'Intensifier': 0, 'Emphatic': 1}},\n",
    "    'Model_3': {'labels_to_include': ['Etymological', 'Intensifier'], 'label_mapping': {'Etymological': 2, 'Intensifier': 0}},\n",
    "    'Model_4': {'labels_to_include': ['Etymological', 'Emphatic'], 'label_mapping': {'Etymological': 2, 'Emphatic': 1}},\n",
    "    'Model_5': {'labels_to_include': ['Etymological', 'Intensifier', 'Emphatic'], 'label_mapping': {'Etymological': 2, 'Intensifier': 0, 'Emphatic': 0}},\n",
    "    'Model_6': {'labels_to_include': ['Etymological', 'Intensifier', 'Emphatic'], 'label_mapping': {'Etymological': 2, 'Intensifier': 0, 'Emphatic': 2}},\n",
    "}\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased', do_lower_case=True)\n",
    "\n",
    "def tokenize_data(sentences, labels):\n",
    "    inputs = tokenizer(\n",
    "        list(sentences),\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LEN,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return inputs['input_ids'], inputs['attention_mask'], torch.tensor(labels)\n",
    "\n",
    "def create_dataloader(inputs, masks, labels, sampler):\n",
    "    data = TensorDataset(inputs, masks, labels)\n",
    "    return DataLoader(data, sampler=sampler(data), batch_size=BATCH_SIZE)\n",
    "\n",
    "def train_and_evaluate(model_name, label_config):\n",
    "    print(f\"\\nTraining {model_name}...\\n\")\n",
    "    df_model = df.copy()\n",
    "    df_model = df_model[df_model['sentido'].isin(label_config['labels_to_include'])]\n",
    "    df_model['label_mapped'] = df_model['sentido'].map(label_config['label_mapping'])\n",
    "    df_model.dropna(subset=['text_literalmente', 'label_mapped'], inplace=True)\n",
    "    sentences = df_model['text_literalmente'].tolist()\n",
    "    labels = df_model['label_mapped'].tolist()\n",
    "    \n",
    "    # Apply Random Oversampling\n",
    "    temp_df = pd.DataFrame({'text': sentences, 'label': labels})\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_resampled, y_resampled = ros.fit_resample(temp_df[['text']], temp_df['label'])\n",
    "    sentences_resampled = X_resampled['text'].tolist()\n",
    "    labels_resampled = y_resampled.tolist()\n",
    "    \n",
    "    # Split dataset\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        sentences_resampled, labels_resampled, test_size=0.2, random_state=42, stratify=labels_resampled)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "    \n",
    "    print(f\"Number of training samples: {len(X_train)}\")\n",
    "    print(f\"Number of validation samples: {len(X_val)}\")\n",
    "    print(f\"Number of test samples: {len(X_test)}\")\n",
    "    \n",
    "    train_inputs, train_masks, train_labels = tokenize_data(X_train, y_train)\n",
    "    val_inputs, val_masks, val_labels = tokenize_data(X_val, y_val)\n",
    "    test_inputs, test_masks, test_labels = tokenize_data(X_test, y_test)\n",
    "    \n",
    "    train_dataloader = create_dataloader(train_inputs, train_masks, train_labels, RandomSampler)\n",
    "    val_dataloader = create_dataloader(val_inputs, val_masks, val_labels, SequentialSampler)\n",
    "    test_dataloader = create_dataloader(test_inputs, test_masks, test_labels, SequentialSampler)\n",
    "    \n",
    "    num_labels = len(set(label_config['label_mapping'].values()))\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        'dccuchile/bert-base-spanish-wwm-uncased',\n",
    "        num_labels=num_labels,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "    )\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=LR, eps=EPS)\n",
    "    total_steps = len(train_dataloader) * EPOCHS // ACCUMULATION_STEPS\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    \n",
    "    best_val_accuracy = 0\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False)\n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            batch_input_ids = batch[0].to(device)\n",
    "            batch_input_mask = batch[1].to(device)\n",
    "            batch_labels = batch[2].to(device)\n",
    "            outputs = model(batch_input_ids, attention_mask=batch_input_mask, labels=batch_labels)\n",
    "            loss = outputs.loss / ACCUMULATION_STEPS\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            if (step + 1) % ACCUMULATION_STEPS == 0 or (step + 1) == len(train_dataloader):\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            progress_bar.set_postfix({'Loss': loss.item() * ACCUMULATION_STEPS})\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_preds, val_labels_list = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                batch_input_ids = batch[0].to(device)\n",
    "                batch_input_mask = batch[1].to(device)\n",
    "                batch_labels = batch[2].to(device)\n",
    "                outputs = model(batch_input_ids, attention_mask=batch_input_mask)\n",
    "                logits = outputs.logits\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels_list.extend(batch_labels.cpu().numpy())\n",
    "        val_accuracy = np.mean(np.array(val_preds) == np.array(val_labels_list))\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS} - Average Training Loss: {avg_train_loss:.4f} - Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            epochs_no_improve = 0\n",
    "            model_name_hub = f'onekat/{model_name}'\n",
    "            model_card_filename = f\"README_{model_name}.md\"\n",
    "            \n",
    "            inverse_label_mapping = {v: k for k, v in label_config['label_mapping'].items()}\n",
    "            target_names = [inverse_label_mapping[i] for i in range(num_labels)]\n",
    "            val_classification_report = classification_report(\n",
    "                val_labels_list, val_preds, target_names=target_names, output_dict=True\n",
    "            )\n",
    "            \n",
    "            # Save README\n",
    "            with open(model_card_filename, \"w\") as f:\n",
    "                f.write(f\"# {model_name}\\n\")\n",
    "                f.write(f\"Validation Accuracy: {val_accuracy:.4f}\\n\")\n",
    "                f.write(f\"Classification Report:\\n{val_classification_report}\\n\")\n",
    "            \n",
    "            # Push model to Hugging Face Hub\n",
    "            model.push_to_hub(model_name_hub, token='YOUR_HF_TOKEN')\n",
    "            tokenizer.push_to_hub(model_name_hub, token='YOUR_HF_TOKEN')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}.\")\n",
    "                break\n",
    "    \n",
    "    print(f\"Training of {model_name} completed.\")\n",
    "\n",
    "# Train all models\n",
    "for model_name, label_config in model_label_configs.items():\n",
    "    train_and_evaluate(model_name, label_config)\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
